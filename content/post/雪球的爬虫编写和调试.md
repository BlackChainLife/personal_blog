---
title: "雪球的爬虫编写和调试"
date: 2024-09-20 16:40
# weight: 1
# aliases: ["/first"]
tags: ["爬虫", "scrapy", "python"]
author: "Me"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "使用Scrapy爬取雪球数据，获取博主全量或者近期的数据。熟练使用scrapy，了解其原理"
canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/BlackChainLife/personal_blog/tree/master/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

# 需要解决的问题(需求)
提取目前雪球和微信关注的不同品种、不同交易技术的博主，提取作者文中提到的关键词，生成常用词并且看看哪些词语重合度很高，个人针对性的去学习。使用自然语言处理、词云生成

# 获取关注的需求博主在雪球发表的所有内容
1. 获取发布的专栏数据
2. 获取发布日常内容

# 比如爬取持有封基的数据，全部数据共计20277条数据

1. 数据量很大注意雪球的爬虫机制，避免直接被禁掉IP地址，需要增加时间的间隔。
2. 数据也包含了不同的类型，比如有原发布、问答、热门、长文、交易、收藏等。最好的方式在爬取数据时，去掉收藏的数据即可。

【非常重要】如何部署雪球的爬虫到多个服务器上，如何传递额外的参数

1. 雪球的token需要手工获取才能正常运行
2. 爬取的博主的用户id可配置
3. 爬虫支持爬取全部的数据和最新的数据，**分别编写single、latest 来获取全量和最新的数据**

# 获取UP主发布文章的接口清单

从以下链接获取数据并保存到数据库中：

https://xueqiu.com/v4/statuses/user_timeline.json?page=1&user_id=6146592061

**Request**

| 请求参数 | 是否必须 | 参数值                                          | 备注                       |      |
| :------- | -------- | ----------------------------------------------- | -------------------------- | ---- |
| page     | 必须     | 页码                                            | 不用添SIZE默认返回20条数据 |      |
| user_id  | 必须     | 用户编号                                        |                            |      |
| type     | 非必须   | 0，原发布；1，长文；4，问答；9，热门；4，交易； |                            |      |
| -        | 非必须   | 没找到是什么值                                  |                            |      |
|          |          |                                                 |                            |      |

**Response**

| 响应字段    | 字段描述                                        | 其他     |
| :---------- | ----------------------------------------------- | -------- |
| description | 具体的内容                                      |          |
| like_count  | 收藏数量                                        | 其他说明 |
| id          | 文章的编码                                      |          |
| user_id     | 用户编号                                        |          |
| created_at  | unix 的创建时间                                 |          |
| type        | 0，原发布；1，长文；4，问答；9，热门；4，交易； |          |
| target      | 文章的链接，需要拼凑后进行使用                  |          |
| reply_count | 评论数量                                        |          |
| raw_title   | 文章的标题                                      |          |
|             |                                                 |          |



# 使用cursor AI 进行脚本开发的需求和需求描述

## 服务器报错`403 Forbidden`  使用动态代理或者静态的代理解决问题

## scrapy 的爬虫如何进行调试?

```PYTHON
from scrapy.shell import inspect_response

# 使用inspect_response，进入到shell中进行调试，如何获取resposne中的数据提取代码等

class MySpider(spider.Spider)
    def parse(self, response):
        inspect_response(response, self)
 ....
```

## 如何在scrapy中增加代理？为了解决服务器返回403的错误。在Middleware发起请求时，新增代理。在settings.py中配置代理的地址，如果是动态代理的话，参考此方法即可。

```PYTHON
import random
class ProxyMiddleware:
    def __init__(self, proxies):
        self.proxies = proxies
    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        proxies = settings.getlist('PROXY_LIST')
        return cls(proxies)

    def process_request(self, request, spider):
        proxy = random.choice(self.proxies)
        request.meta['proxy'] = proxy
        
# settings.py

# 代理列表
PROXY_LIST = [
    'http://proxy1.example.com:8080',
    'http://proxy2.example.com:8080',
    # 添加更多代理
]

# 启用自定义代理中间件
DOWNLOADER_MIDDLEWARES = {
    'your_project_name.middlewares.ProxyMiddleware': 543,
}
```

## 如何在scrapy中设置日志的级别？

```python
# settings.py
# 设置日志级别为 DEBUG，记录更多信息
LOG_LEVEL = 'DEBUG'
# 将日志输出到文件
LOG_FILE = 'scrapy_spider.log'

```

## 如何在scrapy中自动新增页码

```python

import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['http://example.com/page/1']

    def parse(self, response):
        # 解析当前页面的内容
        for item in response.css('div.item'):
            yield {
                'title': item.css('h2::text').get(),
                'link': item.css('a::attr(href)').get(),
            }

        # 查找下一页的链接
        next_page = response.css('a.next::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```

## scrapy如果发起api的请求，新增user-agent、cookies、referer、content-type 等参数发起请求。需要注意cookies传入参数为dict类型，如果是str类型会直接报错。

```python

class TodaySpider(scrapy.Spider):
    name = "xueqiu_today"
    start_urls = [
        'https://xueqiu.com/v4/statuses/user_timeline.json?page=1&user_id=6146592061'
    ]

    def __init__(self, *args, **kwargs):
        super(TodaySpider, self).__init__(*args, **kwargs)
        self.header = {
            "Content-Type": "application/json;charset=UTF-8",
            "referer": "https://xueqiu.com/u/6146592061",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        }
        self.cookies = {
            "xq_a_token": "a84ed0dd80ff2f18e526520d683fbe6cc6189fc2", 
            "xq_id_token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ1aWQiOjEyODEyNDg5MjAsImlzcyI6InVjIiwiZXhwIjoxNzI2NzEzNDE0LCJjdG0iOjE3MjQ4Mzg4MzM0NDgsImNpZCI6ImQ5ZDBuNEFadXAifQ.Yj9zQa5xkCSqIZoePpjDCV7xMf8XRgbyyTG2lSj_O3oZUYydKL65C-i_OOR9AROFRsF1tlxbHcFgr6Zj3A-Upell4txppzpnt1CAvT4SROKTB1xPb0syf-Rhh2251rZZuNcNMGZoJ5IZIND_qwoWCKx5v0ae0cRq9_Hpzz4ViCo17LATAyLH3UWoHaFvVkAJ1E54q7_JS83gOu0ums2rb1fv4uNfrTQxCEb2Qhj3gk7ykgIocZt2P8sZr37XBrp1cDeXCHl6Svf_8EBE4tRZlcPE0u7wfiltsy9ba7_wrvR077TY6YItpYnLMVUOLwSDERoMq5mqiNx7T7FcuJYduA"
        }

    def start_requests(self) -> Iterable[Request]:
        api_url = "https://xueqiu.com/v4/statuses/user_timeline.json?page={page}&user_id=6146592061"
        for page in range(1, 1001):
            yield scrapy.Request(
                url= api_url.format(page=page), 
                headers=self.header,
                cookies=self.cookies,
                callback=self.parse_api,
                dont_filter=True
                )
    
    def parse_api(self, response):
        # 将响应体解析为 JSON
        # inspect_response(response, self)
        json_data = response.json()
        
        for status in json_data['statuses']:
            self.logger.info(status)
            item = XueqiuItem()
            item['airtcle_id'] = status['id']
            item['title'] = status['title']
            item['source'] = status['source']
            item['user_id'] = status['user_id']
            item['created_at'] = status['created_at']
            item['type'] = status['type']
            item['description'] = status['description']
            item['target'] = status['target']
            item['title'] = status['title']
            item['like_count'] = status['like_count']
            item['reply_count'] = status['reply_count']
            item['content'] = ""

            # 检查 target 字段是否存在值，并发起新的请求
            target_url = status.get('target')
            if target_url:
                request = scrapy.Request(
                    'https://xueqiu.com' + target_url, 
                    headers=self.header, 
                    cookies=self.cookies,
                    callback=self.parse_target
                )
                request.meta['item'] = item  # 将 item 传递给 parse_target 方法
                yield request
            else:
                yield item


    def parse_target(self, response):
        # 处理 target 页面的逻辑
        self.logger.info(f'Parsing target page: {response.url}')
        item = response.meta['item']  # 获取传递的 item
        
        # 解析 target 页面的内容并更新 item
        item['content'] = response.css('.article__bd__detail').get()
        
        yield item
```

## 抓取到的数据怎么更加方便的写入数据库中

> 在 Scrapy 中，你可以使用 Item 和 Pipeline 来保存抓取的数据到数据库。以下是一个完整的示例，展示如何将抓取的数据保存到 MySQL 数据库。
> ### 1. 定义 Item
> 首先，定义一个 Item 来表示你要抓取的数据。在 items.py 文件中添加以下内容：
> ### 2. 创建 Pipeline
> 接下来，创建一个 Pipeline 来处理 Item 并保存到数据库。在 pipelines.py 文件中添加以下内容：
> ### 3. 配置 settings.py
> 在 settings.py 文件中配置数据库连接信息和启用 Pipeline：
> ### 4. 编写爬虫
> 最后，编写爬虫代码来抓取数据并生成 Item。以下是一个示例爬虫代码：

## 为了防止爬虫被封禁，可以在执行一定数量的请求后暂停一段时间。你可以使用 Scrapy 的 DOWNLOAD_DELAY 设置来控制请求之间的延迟，或者使用自定义的中间件来实现更复杂的请求控制逻辑。

```python
import time

class CustomDownloaderMiddleware:
    def __init__(self):
        self.request_count = 0
        self.pause_after = 1000
        self.pause_duration = 120  # 2分钟

    def process_request(self, request, spider):
        self.request_count += 1
        if self.request_count % self.pause_after == 0:
            spider.logger.info(f'Pausing for {self.pause_duration} seconds after {self.request_count} requests')
            time.sleep(self.pause_duration)
        return None

```

## 如何更加便捷的获取response中的链接的内容，并且把子链接的内容添加到原内容上

> 在 Scrapy 中，request.meta 是一个字典，用于在请求之间传递自定义数据。当你需要在不同的请求或回调函数之间共享数据时，可以使用 request.meta。
>
> ### 详细解释
>
> request.meta：
>
> 这是一个字典，可以在发起请求时设置，并在回调函数中访问。
>
> 它用于在请求之间传递自定义数据，例如 Item 对象、状态信息等。
>
> ### 为什么需要把 Item 保存到 request.meta 中
>
> 1. 数据传递：
>
> 当你在 parse_api 方法中生成一个 Item 对象，并需要将其传递给另一个回调函数（如 parse_target）时，可以将 Item 保存到 request.meta 中。
>
> 这样，在 parse_target 方法中，你可以从 response.meta 中获取这个 Item 对象，并继续处理。
>
> 2. 避免重复解析：
>
> 通过将 Item 保存到 request.meta 中，可以避免在多个回调函数中重复解析相同的数据。

```python

       def parse_api(self, response):
        # 将响应体解析为 JSON
        json_data = response.json()
        
        for status in json_data['statuses']:
            self.logger.info(status)
            item = XueqiuItem()
            item['airtcle_id'] = status['id']
            item['title'] = status['title']
            item['source'] = status['source']
            item['user_id'] = status['user_id']
            item['created_at'] = status['created_at']
            item['type'] = status['type']
            item['description'] = status['description']
            item['target'] = status['target']
            item['like_count'] = status['like_count']
            item['reply_count'] = status['reply_count']
            item['content'] = ""
            
            # 检查 target 字段是否存在值，并发起新的请求
            target_url = status.get('target')
            if target_url:
                request = scrapy.Request(
                    'https://xueqiu.com' + target_url, 
                    headers=self.header, 
                    cookies=self.cookies,
                    callback=self.parse_target
                )
                request.meta['item'] = item  # 将 item 传递给 parse_target 方法
                yield request
            else:
                yield item

    def parse_target(self, response):
        # 处理 target 页面的逻辑
        self.logger.info(f'Parsing target page: {response.url}')
        item = response.meta['item']  # 获取传递的 item
        
        # 解析 target 页面的内容并更新 item
        item['content'] = response.css('div.content::text').get()
        
        yield item
```

## 通过pipline把不同的数据写入不同数据库，出现数据无法正常解析的情况。（未搞懂）

在process_item 函数判断item的类型分别进行处理
